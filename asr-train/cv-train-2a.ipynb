{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2Tokenizer, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "c:\\Users\\evann\\anaconda3\\envs\\asr\\Lib\\site-packages\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py:720: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "proj_dir = r\"C:\\Users\\evann\\Documents\\GitHub\\ASR\"\n",
    "csv_file = os.path.join(proj_dir, \"datasets\", \"cv-valid-train.csv\")\n",
    "audio_dir = os.path.join(proj_dir, \"datasets\")\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the sample rate is 16kHz and audio is in the correct format\n",
    "def preprocess_audio(file_path):\n",
    "    # Load the MP3 file and resample it to 16kHz\n",
    "    waveform, sample_rate = librosa.load(file_path, sr=16000) \n",
    "    \n",
    "    # Convert the waveform to a PyTorch tensor (as librosa returns a numpy array)\n",
    "    waveform = torch.tensor(waveform)\n",
    "\n",
    "    audio = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_values = audio.input_values.squeeze() \n",
    "    \n",
    "    return input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to preprocess the text labels\n",
    "def preprocess_text(text):\n",
    "    labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids.squeeze()\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map each sample to audio and text\n",
    "def map_to_dataset(row):\n",
    "    # Load the audio and preprocess it\n",
    "    audio_path = os.path.join(audio_dir, row['filename'])\n",
    "    input_values = preprocess_audio(audio_path)\n",
    "    labels = preprocess_text(row['text'])\n",
    "  \n",
    "    return {\n",
    "        'input_values': input_values,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mapping to the dataframe\n",
    "df = df.apply(map_to_dataset, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into 70% training and 30% validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch Dataset for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_values = torch.tensor(self.dataset[idx][\"input_values\"])\n",
    "        labels = torch.tensor(self.dataset[idx][\"labels\"])\n",
    "        return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(SpeechDataset(train_df), batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(SpeechDataset(val_df), batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Set up optimizer and learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the model and save as \"wav2vec2-large-960h-cv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_values = batch[\"input_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_values = batch[\"input_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_values, labels=labels)\n",
    "            running_val_loss += outputs.loss.item()\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_loss_list.append(avg_train_loss)  # Store the training loss for this epoch\n",
    "    \n",
    "    # Calculate average validation loss for the epoch\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    val_loss_list.append(avg_val_loss)  # Store the validation loss for this epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"wav2vec2-large-960h-cv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_dataset = load_dataset(\"common_voice\", \"en\", split='test[:100%]')\n",
    "\n",
    "# Preprocess the test dataset\n",
    "test_dataset = test_dataset.map(preprocess_data, remove_columns=[\"audio\", \"sentence\"])\n",
    "\n",
    "# Create DataLoader for the test dataset\n",
    "test_loader = DataLoader(SpeechDataset(test_dataset), batch_size=16)\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    input_values = batch[\"input_values\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values, labels=labels)\n",
    "        predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert predicted_ids to text and compare with actual labels\n",
    "    # Compute metrics like WER (Word Error Rate) for evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss curves\n",
    "plt.plot(train_loss_list, label=\"Training Loss\")\n",
    "plt.plot(val_loss_list, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
