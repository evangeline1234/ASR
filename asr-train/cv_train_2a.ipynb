{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NHkOkAMYb96h"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\evann\\anaconda3\\envs\\asr\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "import torch\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, get_scheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MLyXgWx8dQnA"
      },
      "outputs": [],
      "source": [
        "# Get audio directory and cv-valid-train.csv\n",
        "audio_dir = \"../datasets/\"\n",
        "train_df = pd.read_csv('../datasets/cv-valid-train.csv')\n",
        "train_df = train_df[['filename', 'text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J31AlFsZdmkW",
        "outputId": "dc03bc95-a067-41d7-d583-a52253d9af78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the processor and model\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBpT10MtzVl6"
      },
      "source": [
        "### This section is to preprocess the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COY_Cideb96i"
      },
      "outputs": [],
      "source": [
        "# 1. Optimized Audio Preprocessing (Using torchaudio + GPU Support)\n",
        "def preprocess_audio(file_path):\n",
        "    \"\"\"Load, resample, and convert audio to tensor format\"\"\"\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "    # Resample if necessary\n",
        "    if sample_rate != 16000:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
        "\n",
        "    waveform = waveform.squeeze()  # Remove extra channel dimension if necessary\n",
        "    waveform = waveform.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Move to GPU if available\n",
        "\n",
        "    # Process with Hugging Face Wav2Vec2 processor\n",
        "    audio = processor(waveform.to(\"cpu\"), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    return audio.input_values.squeeze()  # Return processed tensor\n",
        "\n",
        "# 2. Optimized Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Tokenize text labels\"\"\"\n",
        "    # Convert text to uppercase as processor's vocab labels are uppercase\n",
        "    labels = processor.tokenizer(text.upper(), return_tensors=\"pt\").input_ids.squeeze()\n",
        "    return labels\n",
        "\n",
        "# 3. Mapping Function for Multiprocessing (no lambda here)\n",
        "def map_to_dataset(row, audio_dir):\n",
        "    \"\"\"Process audio and text for parallel execution\"\"\"\n",
        "    #print(f\"Processing row: {row['filename']}\")\n",
        "    audio_path = os.path.join(audio_dir, row['filename'])\n",
        "    input_values = preprocess_audio(audio_path)  # Audio Processing\n",
        "    labels = preprocess_text(row['text'])  # Text Tokenization (of given transcript)\n",
        "\n",
        "    return {'input_values': input_values, 'labels': labels}\n",
        "\n",
        "# 4. Parallel Processing using ProcessPoolExecutor\n",
        "def preprocess_data_parallel(data, audio_dir, num_workers=4):\n",
        "    \"\"\"Process data in parallel using multiprocessing\"\"\"\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        results = list(executor.map(process_row, data, [audio_dir]*len(data)))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Helper function to avoid lambda\n",
        "def process_row(row, audio_dir):\n",
        "    return map_to_dataset(row, audio_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "laLfzNF2k0dQ"
      },
      "outputs": [],
      "source": [
        "# Process a subset of 10k rows of data\n",
        "df_subset = train_df[:10000].copy()\n",
        "\n",
        "# Convert DataFrame to a list of dictionaries\n",
        "mp.set_start_method('spawn', force=True)\n",
        "dataset = df_subset.to_dict(orient='records')\n",
        "train_data, val_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
        "\n",
        "# Process training and validation data\n",
        "train_processed_data = preprocess_data_parallel(train_data, audio_dir)\n",
        "val_processed_data = preprocess_data_parallel(val_data, audio_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsmL8DmOsyA9",
        "outputId": "8732539a-4ae4-4695-ce02-4c7c66acc1f7"
      },
      "outputs": [],
      "source": [
        "# Save training and validation data\n",
        "#torch.save(train_processed_data, '/content/drive/My Drive/asr-train/train_processed_data.pt')\n",
        "#torch.save(val_processed_data, '/content/drive/My Drive/asr-train/val_processed_data.pt')\n",
        "#print(\"✅ Processed dataset saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLlBzPG3cQem"
      },
      "source": [
        "\n",
        "### This section is to fine-tune the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "nOflU6-au3oc",
        "outputId": "80972fe0-28c7-4f8e-f3e8-054f2caa1f47"
      },
      "outputs": [],
      "source": [
        "# Load saved data (instead of reprocessing large amounts of data every session)\n",
        "#train_processed_data = torch.load('/content/drive/My Drive/train_processed_data.pt')\n",
        "#val_processed_data = torch.load('/content/drive/My Drive/val_processed_data.pt')\n",
        "#print(\"✅ Processed dataset loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdItNDdgvhsQ"
      },
      "outputs": [],
      "source": [
        "def data_collator(batch):\n",
        "    # Pad audio sequences (input_values) in the batch\n",
        "    input_values = [item['input_values'] for item in batch]\n",
        "    input_values_padded = pad_sequence(input_values, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad text sequences (labels) in the batch\n",
        "    labels = [item['labels'] for item in batch]\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
        "\n",
        "    return {'input_values': input_values_padded, 'labels': labels_padded}\n",
        "\n",
        "\n",
        "class AudioTextDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Each item is a dictionary {'input_values': ..., 'labels': ...}\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7rCLpegcf8g"
      },
      "outputs": [],
      "source": [
        "# Create DataLoader for batching and shuffling\n",
        "train_dataset = AudioTextDataset(train_processed_data)\n",
        "val_dataset = AudioTextDataset(val_processed_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qttvL0yFvoGo"
      },
      "outputs": [],
      "source": [
        "# Optimizer and learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Learning rate scheduler\n",
        "num_epochs = 5\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "num_warmup_steps = int(0.1 * num_training_steps)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdK_2CoPvoI-",
        "outputId": "667d4aa4-e22d-45fc-cb0d-ba56af596adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-52af37fa7dca>:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Epoch 1/5:   0%|          | 0/1750 [00:00<?, ?it/s]<ipython-input-9-52af37fa7dca>:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/5: 100%|██████████| 1750/1750 [27:41<00:00,  1.05it/s, loss=113]\n",
            "<ipython-input-9-52af37fa7dca>:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Training Loss: -115.7268\n",
            "Validation Loss: -133.2053\n",
            "Checkpoint saved: checkpoints/model_epoch_1.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/5: 100%|██████████| 1750/1750 [13:44<00:00,  2.12it/s, loss=-370]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5\n",
            "Training Loss: inf\n",
            "Validation Loss: -133.4147\n",
            "Checkpoint saved: checkpoints/model_epoch_2.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/5: 100%|██████████| 1750/1750 [13:32<00:00,  2.15it/s, loss=-224]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5\n",
            "Training Loss: -162.2955\n",
            "Validation Loss: -143.6807\n",
            "Checkpoint saved: checkpoints/model_epoch_3.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/5: 100%|██████████| 1750/1750 [13:28<00:00,  2.16it/s, loss=-161]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5\n",
            "Training Loss: -169.4942\n",
            "Validation Loss: -135.3436\n",
            "Checkpoint saved: checkpoints/model_epoch_4.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/5: 100%|██████████| 1750/1750 [13:28<00:00,  2.16it/s, loss=-323]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5\n",
            "Training Loss: -176.0803\n",
            "Validation Loss: -138.2690\n",
            "Checkpoint saved: checkpoints/model_epoch_5.pth\n"
          ]
        }
      ],
      "source": [
        "# Move model to device (GPU or CPU)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "# Gradient checkpointing to reduce memory usage\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# GradScaler for mixed precision training\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Directory to save model checkpoints\n",
        "checkpoint_dir = \"./test_checkpoints/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        torch.cuda.empty_cache() # Free memory before processing each batch\n",
        "        input_values = batch[\"input_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision\n",
        "        with autocast():\n",
        "            # Forward pass\n",
        "            outputs = model(input_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevent exploding gradient by clipping\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_values = batch[\"input_values\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(input_values, labels=labels)\n",
        "                running_val_loss += outputs.loss.item()\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = running_train_loss / len(train_loader)\n",
        "    train_loss_list.append(avg_train_loss)\n",
        "\n",
        "    # Calculate average validation loss for the epoch\n",
        "    avg_val_loss = running_val_loss / len(val_loader)\n",
        "    val_loss_list.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # **Save model checkpoint**\n",
        "    checkpoint_path = f\"{checkpoint_dir}/model_epoch_{epoch+1}.pth\"\n",
        "    torch.save({\n",
        "        'epoch': epoch+1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scaler_state_dict': scaler.state_dict(),\n",
        "        'train_loss': avg_train_loss,\n",
        "        'val_loss': avg_val_loss\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved: {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6lpl1diVvx-C"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./wav2vec2-large-960h-cv\")\n",
        "print(\"✅ Fine-tuned model saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDVnpLGamZaC"
      },
      "outputs": [],
      "source": [
        "# Function to transcribe audio for test dataset using fine-tuned model\n",
        "def transcribe_audio(file_path):\n",
        "    audio_path = os.path.join(audio_dir, file_path)\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    # Resample if necessary\n",
        "    if sample_rate != 16000:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
        "\n",
        "    waveform = waveform.squeeze()\n",
        "    waveform = waveform.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Convert to tensor using processor\n",
        "    input_values = processor(waveform.to(\"cpu\"), return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Get predicted transcription\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)[0]\n",
        "    print(f\"Audio for {file_path} transcribed!\")\n",
        "\n",
        "    return transcription\n",
        "\n",
        "# Function to transcribe a single file\n",
        "def transcribe_file(index, row):\n",
        "    file_path = row[\"filename\"]\n",
        "    transcription = transcribe_audio(file_path)  # I/O operation\n",
        "    return index, transcription.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This section is to evaluate model on cv-valid-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cd0zH_dWhryP"
      },
      "outputs": [],
      "source": [
        "# Load test data\n",
        "test_df = pd.read_csv('../datasets/cv-valid-test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J54F4fs9x1Ag",
        "outputId": "8ccd9390-a6bb-4cfc-a1ce-cd174bcd4446"
      },
      "outputs": [],
      "source": [
        "# Use ThreadPoolExecutor for parallel processing (best for I/O-bound tasks)\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust based on CPU\n",
        "    results = list(executor.map(lambda x: transcribe_file(*x), test_df.iterrows()))\n",
        "\n",
        "# Update DataFrame with transcriptions\n",
        "for index, transcription in results:\n",
        "    test_df.at[index, 'predicted_transcription'] = transcription\n",
        "\n",
        "# Save results\n",
        "test_df.to_csv('../datasets/cv-valid-test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk1eyVB3wVqF",
        "outputId": "a7bc96da-b56e-4aee-cc35-9480cb18d958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cv-valid-test:\n",
            "\n",
            "Word Error Rate (WER): 7.96%\n",
            "Character Error Rate (CER): 3.49%\n"
          ]
        }
      ],
      "source": [
        "from jiwer import wer, cer\n",
        "\n",
        "# Calculate WER and CER\n",
        "wer_value = wer(test_df[\"text\"].tolist(), test_df[\"predicted_transcription\"].tolist())\n",
        "cer_value = cer(test_df[\"text\"].tolist(), test_df[\"predicted_transcription\"].tolist())\n",
        "print(\"cv-valid-test:\")\n",
        "print(f\"Word Error Rate (WER): {wer_value * 100:.2f}%\")\n",
        "print(f\"Character Error Rate (CER): {cer_value * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating the model on cv-valid-test:\n",
        "\n",
        "Word Error Rate (WER): 7.96% <br>\n",
        "Character Error Rate (CER): 3.49%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj0j2virx2tx"
      },
      "source": [
        "### For Question  4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVkSegu_v9yE"
      },
      "outputs": [],
      "source": [
        "# Use model to predict transcriptions on cv-valid-dev\n",
        "dev_df = pd.read_csv('../datasets/cv-valid-dev.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOaUknIrVPqv",
        "outputId": "f4e429be-53d6-4a31-d4f3-655604c9f2a0"
      },
      "outputs": [],
      "source": [
        "# Use ThreadPoolExecutor for parallel processing (best for I/O-bound tasks)\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust based on CPU\n",
        "    results = list(executor.map(lambda x: transcribe_file(*x), dev_df.iterrows()))\n",
        "\n",
        "# Update DataFrame with transcriptions\n",
        "for index, transcription in results:\n",
        "    dev_df.at[index, 'predicted_transcription'] = transcription\n",
        "\n",
        "# Save results\n",
        "dev_df.to_csv('../datasets/cv-valid-dev.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOO26xxLySt2",
        "outputId": "53dcc318-c45d-47d8-86dc-b13edae5d316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results from Task 2a for the cv-valid-dev (pretrained model) :\n",
            "Word Error Rate (WER): 10.81%\n",
            "Character Error Rate (CER): 4.52%\n",
            "\n",
            "Results from Task 4 for the cv-valid-dev (finetuned model) :\n",
            "Word Error Rate (WER): 8.03%\n",
            "Character Error Rate (CER): 3.43%\n"
          ]
        }
      ],
      "source": [
        "# Calculate WER and CER of generated text from Task 2 and this task\n",
        "\n",
        "# Remove any empty rows\n",
        "dev_df = dev_df.dropna(subset=['generated_text','predicted_transcription'])\n",
        "\n",
        "# Generated text from pretrained model\n",
        "pretrained_wer_value = wer(dev_df[\"text\"].tolist(), dev_df[\"generated_text\"].tolist())\n",
        "pretrained_cer_value = cer(dev_df[\"text\"].tolist(), dev_df[\"generated_text\"].tolist())\n",
        "print(\"Results from Task 2a for the cv-valid-dev (pretrained model) :\")\n",
        "print(f\"Word Error Rate (WER): {pretrained_wer_value * 100:.2f}%\")\n",
        "print(f\"Character Error Rate (CER): {pretrained_cer_value * 100:.2f}%\")\n",
        "\n",
        "# Predicted transcription from pretrained model\n",
        "finetuned_wer_value = wer(dev_df[\"text\"].tolist(), dev_df[\"predicted_transcription\"].tolist())\n",
        "finetuned_cer_value = cer(dev_df[\"text\"].tolist(), dev_df[\"predicted_transcription\"].tolist())\n",
        "print(\"\\nResults from Task 4 for the cv-valid-dev (finetuned model) :\")\n",
        "print(f\"Word Error Rate (WER): {finetuned_wer_value * 100:.2f}%\")\n",
        "print(f\"Character Error Rate (CER): {finetuned_cer_value * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results from Task 2a for the cv-valid-dev (pretrained model):\n",
        "\n",
        "Word Error Rate (WER): 10.81% <br>\n",
        "Character Error Rate (CER): 4.52%\n",
        "\n",
        "<br>\n",
        "Results from Task 4 for the cv-valid-dev (finetuned model):\n",
        "\n",
        "Word Error Rate (WER): 8.03% <br>\n",
        "Character Error Rate (CER): 3.43%"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "asr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
